{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as pyTorch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as function\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "from utils import plotLearning\n",
    "from gym import wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q Network\n",
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, learning_rate, input_dimensions, fc1_dimensions, fc2_dimensions, num_actions):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.input_dimensions = input_dimensions\n",
    "        self.fc1_dimensions = fc1_dimensions\n",
    "        self.fc2_dimensions = fc2_dimensions\n",
    "        self.num_actions = num_actions\n",
    "        self.fc1 = nn.Linear(*self.input_dimensions, self.fc1_dimensions)\n",
    "        self.fc2 = nn.Linear(self.fc1_dimensions, self.fc2_dimensions)\n",
    "        self.fc3 = nn.Linear(self.fc2_dimensions, self.num_actions)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "        #CPU usage\n",
    "        self.device = pyTorch.device('cpu')\n",
    "        self.to(self.device)\n",
    "        \n",
    "    \n",
    "    def forward(self, observation):\n",
    "        # For this Open AI Gym environment, we don't have to do flattening\n",
    "        state = pyTorch.Tensor(observation).to(self.device)\n",
    "        x = function.relu(self.fc1(state))\n",
    "        x = function.relu(self.fc2(x))\n",
    "        actions = self.fc3(x)\n",
    "        \n",
    "        return actions    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What makes DQN powerful are the max memory and batch size.\n",
    "# As the agent plays the game, it stores all its actions, state, \n",
    "# reward, new station transitions and done flags in its memory\n",
    "# This allows it to sample from that memory to collect all of the experiences its used\n",
    "# to update its values for these actions\n",
    "\n",
    "#params\n",
    "# gamma - discount factor of future rewards\n",
    "# epsilon - how greedy\n",
    "# eps_end - Final epsilon threshold we want to reach\\\n",
    "# eps_dec - Decrease rate for epsilon\n",
    "\n",
    "class Agent(object):\n",
    "    \n",
    "    def __init__(self, gamma, epsilon, learning_rate, input_dimensions, batch_size, number_actions, \n",
    "                     max_memory_size=1000000, eps_end=0.01, eps_dec=0.996):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "        self.learning_rate = learning_rate\n",
    "        self.input_dimensions = input_dimensions\n",
    "        self.batch_size = batch_size\n",
    "        self.number_actions = number_actions\n",
    "        self.action_space = [i for i in range(number_actions)]\n",
    "        self.max_memory_size = max_memory_size\n",
    "        self.Q_eval = DeepQNetwork(learning_rate, input_dimensions, num_actions=self.number_actions,\n",
    "                             fc1_dimensions=256, fc2_dimensions=256)\n",
    "        self.state_memory = np.zeros((self.max_memory_size, *input_dimensions))\n",
    "        self.new_state_memory = np.zeros((self.max_memory_size, *input_dimensions))\n",
    "        self.action_memory = np.zeros((self.max_memory_size, self.number_actions), dtype=np.bool)\n",
    "        self.reward_memory = np.zeros(self.max_memory_size)\n",
    "        self.terminal_memory = np.zeros(self.max_memory_size, dtype=np.bool)\n",
    "        self.memory_counter = 0\n",
    "        \n",
    "    def store_transition(self, state, action, reward, state_, terminal):\n",
    "            index = self.memory_counter % self.max_memory_size\n",
    "            self.state_memory[index] = state\n",
    "            actions = np.zeros(self.number_actions)\n",
    "            actions[action] = 1.0\n",
    "            self.action_memory[index] = actions\n",
    "            self.reward_memory[index] = reward\n",
    "            self.terminal_memory[index] = 1 - terminal\n",
    "            self.new_state_memory[index] = state_\n",
    "            self.memory_counter = self.memory_counter + 1\n",
    "            \n",
    "    def choose_action(self, observation):\n",
    "            rand = np.random.random()\n",
    "            if rand < self.epsilon:\n",
    "                action = np.random.choice(self.action_space)\n",
    "            else:\n",
    "                actions = self.Q_eval.forward(observation)\n",
    "                action = pyTorch.argmax(actions).item()\n",
    "            return action\n",
    "\n",
    "    def learn(self):\n",
    "            if self.memory_counter > self.batch_size:\n",
    "                self.Q_eval.optimizer.zero_grad()\n",
    "                max_memory = self.memory_counter if self.memory_counter < self.max_memory_size else self.max_memory_size\n",
    "            \n",
    "                batch = np.random.choice(max_memory, self.batch_size)\n",
    "                state_batch = self.state_memory[batch]\n",
    "                action_batch = self.action_memory[batch]\n",
    "                reward_batch = self.reward_memory[batch]\n",
    "                terminal_batch = self.terminal_memory[batch]\n",
    "                new_state_batch = self.new_state_memory[batch]\n",
    "                action_values = np.array(self.action_space, dtype=np.bool)\n",
    "                action_indices = np.dot(action_batch, action_values)\n",
    "\n",
    "                reward_batch = pyTorch.Tensor(reward_batch).to(self.Q_eval.device)\n",
    "                terminal_batch = pyTorch.Tensor(terminal_batch)\n",
    "\n",
    "                q_eval = self.Q_eval.forward(state_batch).to(self.Q_eval.device)\n",
    "                q_target = self.Q_eval.forward(state_batch).to(self.Q_eval.device)\n",
    "\n",
    "                q_next = self.Q_eval.forward(new_state_batch).to(self.Q_eval.device)\n",
    "\n",
    "                batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "                \n",
    "                target[batch_index, action_indices] = (reward_batch + self.gamma) * pyTorch.max(q_next, dim = 1)[0]*terminal_batch\n",
    "                self.epsilon = self.epsilon* self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "\n",
    "                loss = self.Q_eval.loss(q_target, q_eval).to(self.Q_eval.device)\n",
    "\n",
    "                loss.backward()\n",
    "                self.Q_eval.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 score 0\n",
      "episode 1 score -67.21117215505978\n",
      "episode 2 score -157.21560096732046\n",
      "episode 3 score -175.9185699624876\n",
      "episode 4 score -332.0710388727816\n",
      "episode 5 score -576.2349814715928\n",
      "episode 6 score -191.21855497220469\n",
      "episode 7 score -379.5048136468788\n",
      "episode 8 score -372.0514379968254\n",
      "episode 9 score -357.1679878542182\n",
      "episode 10 score:  19.737969724871718 average score -258.886 epsilon 0.022\n",
      "episode 11 score 19.737969724871718\n",
      "episode 12 score -195.94421505100905\n",
      "episode 13 score -228.7999218135468\n",
      "episode 14 score -195.12377805433235\n",
      "episode 15 score -256.53299405303693\n",
      "episode 16 score -456.9358690377209\n",
      "episode 17 score -449.0479503552436\n",
      "episode 18 score -301.93641669647195\n",
      "episode 19 score -448.02980746525026\n",
      "episode 20 score:  -156.87895007352 average score -298.803 epsilon 0.010\n",
      "episode 21 score -156.87895007352\n",
      "episode 22 score -364.0007008652012\n",
      "episode 23 score -584.77735406636\n",
      "episode 24 score -378.0427975793806\n",
      "episode 25 score -13.084852918151498\n",
      "episode 26 score -154.94514281865077\n",
      "episode 27 score -158.13713472043065\n",
      "episode 28 score -221.04843468583232\n",
      "episode 29 score -131.99669303042518\n",
      "episode 30 score:  -771.6876236400501 average score -301.715 epsilon 0.010\n",
      "episode 31 score -771.6876236400501\n",
      "episode 32 score -578.5881547160361\n",
      "episode 33 score -109.74066745996642\n",
      "episode 34 score -214.08054910279446\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-b95b71831794>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mbrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_transition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0mbrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-99-d563587094cf>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mq_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                 \u001b[0mq_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mq_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_eval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-98-288454960131>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# For this Open AI Gym environment, we don't have to do flattening\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyTorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    brain = Agent(gamma = 0.99, epsilon = 1.0, batch_size = 64, number_actions = 4, input_dimensions = [8],\n",
    "                             learning_rate = 0.003)\n",
    "    scores = []\n",
    "    episode_history = []\n",
    "    number_games = 500\n",
    "    score = 0\n",
    "    \n",
    "    for i in range(number_games):\n",
    "        if i % 10 == 0 and i > 0:\n",
    "            average_score = np.mean(scores[max(0, i - 10): (i + 1)])\n",
    "            print('episode', i, 'score: ', score, 'average score %.3f' % average_score,\n",
    "                 'epsilon %.3f' % brain.epsilon)\n",
    "        else:\n",
    "            print('episode', i, 'score', score)\n",
    "            score = 0\n",
    "            episode_history.append(brain.epsilon)\n",
    "            observation = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = brain.choose_action(observation)\n",
    "                observation_, reward, done, info = env.step(action)\n",
    "                score += reward\n",
    "                brain.store_transition(observation, action, reward, observation_, done)\n",
    "                brain.learn()\n",
    "                observation = observation_\n",
    "            scores.append(score)\n",
    "    x = [i + 1 for i in range(number_games)]\n",
    "    filename = 'lunar_lander.png'\n",
    "    plotLearning(x, scores, eps_history, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
