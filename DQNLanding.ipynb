{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch as pyTorch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as function\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Q Network\n",
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, learning_rate, input_dimensions, fc1_dimensions, fc2_dimensions, num_actions):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.input_dimensions\n",
    "        self.fc1_dimensions = fc1_dimensions\n",
    "        self.fc2_dimensions = fc2_dimensions\n",
    "        self.num_actions = num_actions\n",
    "        self.fc1 = nn.Linear(*self.input_dim, self.fc1_dimensions)\n",
    "        self.fc2 = nn.Linear(self.fc1_dimensions, self.fc2_dimensions)\n",
    "        self.fc3 = nn.Linear(self.fc2_dimensions, self.num_actions)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "        #CPU usage\n",
    "        self.device = pyTorch.device()\n",
    "    \n",
    "    def forward(self, observation):\n",
    "        # For this Open AI Gym environment, we don't have to do flattening\n",
    "        state = pyTorch.Tensor(observation).to(self.device)\n",
    "        x = function.relu(self.fc1(state))\n",
    "        x = function.relu(self.fc2(x))\n",
    "        actions = self.fc3(x)\n",
    "        \n",
    "        return actions    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What makes DQN powerful are the max memory and batch size.\n",
    "# As the agent plays the game, it stores all its actions, state, \n",
    "# reward, new station transitions and done flags in its memory\n",
    "# This allows it to sample from that memory to collect all of the experiences its used\n",
    "# to update its values for these actions\n",
    "\n",
    "#params\n",
    "# gamma - discount factor of future rewards\n",
    "# epsilon - how greedy\n",
    "# eps_end - Final epsilon threshold we want to reach\\\n",
    "# eps_dec - Decrease rate for epsilon\n",
    "\n",
    "class Agent(object):\n",
    "    \n",
    "    def __init__(self, gamma, epsilon, learning_rate, input_dimensions, batch_size, number_actions, \n",
    "                     max_memory_size=1000000, eps_end=0.01, eps_dec=0.996):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_min = eps_min\n",
    "        self.learning_rate = learning_rate\n",
    "        self.input_dimensions = input_dimensions\n",
    "        self.batch_size = batch_size\n",
    "        self.number_actions = number_actions\n",
    "        self.action_space = [i for i in range(number_dactions)]\n",
    "        self.max_memory_size = max_memory_size\n",
    "        self.DeepQNetwork(learning_rate, input_dimensions, num_actions=self.number_actions,\n",
    "                             fc1_dimensions=256, fc2_dimensions=256)\n",
    "        self.state_memory = np.zeros((self.max_memory_size, *input_dims))\n",
    "        self.action_memory = np.zeros((self.max_memory_size, self.number_actions), dtype=np.uint8)\n",
    "        self.reward_memory = np.zeros(self.max_memory_size)\n",
    "        self.terminal_memory = np.zeros(self.max_memory_size, dtype=np.uint8)\n",
    "        self.memory_counter = 0\n",
    "        \n",
    "        def store_transition(self, state, action, reward, state_, terminal):\n",
    "            index = self.memory_counter % self.max_memory_size\n",
    "            self.state_memory[index] = state\n",
    "            actions = np.zeros(self.number_actions)\n",
    "            actions[action] = 1.0\n",
    "            self.action_memory[index] = actions\n",
    "            self.reward_memory[index] = reward\n",
    "            self.terminal_memory[index] = 1 - terminal\n",
    "            self.new_state_memory[index] = state_\n",
    "            self.memory_counter = self.memory_counter + 1\n",
    "            \n",
    "        def choose_action():\n",
    "            rand = np.random.random()\n",
    "            if rand < self.epsilon:\n",
    "                action = np.random.choice(self.action_space)\n",
    "            else:\n",
    "                actions = self.Q_eval.forward(observation)\n",
    "                action = T.argmax(actions).item()\n",
    "            return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
